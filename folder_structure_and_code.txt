(Directory) evaluations/evaluation_outputs
(Directory) evaluations/evaluation_outputs/multi_relation_results
(Directory) evaluations/evaluation_outputs/captured_relations_results
(Directory) evaluations/evaluation_outputs/captured_relations_results/debug_outputs
(Directory) evaluations/kumu_to_pipeline
(Directory) evaluations/kumu_to_pipeline/__pycache__
(Code File) evaluations/kumu_to_pipeline/parser.py


========== START OF evaluations/kumu_to_pipeline/parser.py ==========

import json

def kumu_to_pipeline(file):
    curFile = json.load(open("./inputs/" + file))
    output = {
        "relations": []
    }
    

    for connection in curFile['connections']:
        if connection["direction"] == "directed":
            connection["direction"] = "direct"

        if connection["type"] == "causal":
            connection["type"] = "True"
        elif connection["type"] == "non-causal":
            connection["type"] = "False"

        entry = {
            "VariableOneName": connection['from'],
            "VariableTwoName": connection['to'],
            "RelationshipClassification": connection['direction'],
            "isCausal" : connection['type'],
            "SupportingText": connection['description']
        }
        output['relations'].append(entry)
    with open("./outputs/" + file[:-5] + "_output.json", "w") as f:
        f.write(json.dumps(output, indent=4))



========== END OF evaluations/kumu_to_pipeline/parser.py ==========

(Directory) evaluations/kumu_to_pipeline/outputs
(Directory) evaluations/kumu_to_pipeline/inputs
(Code File) evaluations/__init__.py


========== START OF evaluations/__init__.py ==========



========== END OF evaluations/__init__.py ==========

(Directory) evaluations/pipelines
(Code File) evaluations/pipelines/var_names.py


========== START OF evaluations/pipelines/var_names.py ==========

"""Extracts tuples/lists of variable names without duplicates from input files"""
import json 
import pathlib
import os


def extract_all_unique_pairs(file_path):
    #Load the json file
    # directory = os.getcwd()
    # inputs_path = directory + "/inputs"
    # file_name = "test_paper.json"
    # file_path = os.path.join(inputs_path, file_name)

    with open(file_path, 'r', encoding='utf-8') as file:
        paper = json.load(file)
    #Extract the relationships
    relationships = paper.get("Variables", [])
    #Store unique ordered pairs of variables
    variable_pairs = []
    # Create a set to check for uniqueness
    seen_pairs = set()
    # Iterate through each relationship and extract variables
    for relationship in relationships:
        variable_one = relationship.get("VariableOneName", "")
        variable_two = relationship.get("VariableTwoName", "")

        # Check for uniqueness based on both orders of the pair
        pair_1 = (variable_one, variable_two)
        pair_2 = (variable_two, variable_one)
        if pair_1 not in seen_pairs and pair_2 not in seen_pairs:
            variable_pairs.append([variable_one, variable_two])
            seen_pairs.add(pair_1)
    # Print the resulting list of variable lists
    return variable_pairs

def extract_all_ordered_pairs(data):
    #Extract the relationships
    relationships = data.get("Relations", [])
    #Store unique ordered pairs of variables
    variable_pairs = []
    # Iterate through each relationship and extract variables
    for relationship in relationships:
        variable_one = relationship.get("VariableOneName", "")
        variable_two = relationship.get("VariableTwoName", "")
        variable_pairs.append(variable_one + " -> " + variable_two)
    # Print the resulting list of variable lists
    relations_text = "\n".join(variable_pairs)
    return relations_text

if __name__ == "__main__":
    ans = extract_all_ordered_pairs("../evaluation_datasets/multi_relation_dataset/test_paper.json")
    print(ans)

========== END OF evaluations/pipelines/var_names.py ==========

(Directory) evaluations/pipelines/debug_outputs
(Code File) evaluations/pipelines/__init__.py


========== START OF evaluations/pipelines/__init__.py ==========



========== END OF evaluations/pipelines/__init__.py ==========

(Directory) evaluations/pipelines/__pycache__
(Code File) evaluations/pipelines/w_multi_paper_pipeline.py


========== START OF evaluations/pipelines/w_multi_paper_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

#Setting paths
directory = os.getcwd()
PAPER_SOURCE = "/papers"
PAPER_SOURCE_PATH = directory + PAPER_SOURCE
OUTPUTS_SOURCE = directory + "/outputs"
PAPER_FILENAME = "RuminationandCognitiveDistractionin_10.1007_s10862_015_9510_1.txt"
MODEL_NAME = "gpt-3.5-turbo-1106"
PAPER_PATH = os.path.join(PAPER_SOURCE_PATH, PAPER_FILENAME)

#Configure output parser classes
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    RelationshipClassification: str
    isCausal: str
    SupportingText: str

    @validator("RelationshipClassification")
    def question_ends_with_question_mark(cls, field):
        if field.lower() in {"direct", "inverse", "inconclusive"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

class ListOfRelations(BaseModel):
    Relations: list[SingleRelation]

def extract_relationships(text, verbose = False, model = MODEL_NAME):
    # Add map reduce or some other type of summarization function here.
    processed_text = text

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=ListOfRelations) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    formatting_text = """
    The output should be formatted as a JSON instance that conforms to the JSON schema below.

    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

    Here is the output schema:
    ```
    {"Relationships":[{"properties": {"VariableOneName": {"title": "Variableonename", "type": "string"}, "VariableTwoName": {"title": "Variabletwoname", "type": "string"}, "RelationshipClassification": {"title": "Relationshipclassification", "type": "string"}, "isCausal": {"title": "Iscausal", "type": "string"}, "SupportingText": {"title": "Supportingtext", "type": "string"}}, "required": ["VariableOneName", "VariableTwoName", "RelationshipClassification", "isCausal", "SupportingText"]}]}
    ```"""
    prompt = PromptTemplate(
        template="""
        {text}

        Given the text identify a series of relationships between variables that have a well defined positive direction and a clear textual backing.
        For example, if a text says "We find a strong correlation p < .0001 between a country's per capita income and it's literacy rate" then "Country's per capita income" and "Country's literacy rate" would be good variables. Variables that are implied but not directly stated by the text such as "education infilstructure" should not be included. 
        The RelationshipClassification field can only be 'direct', 'inverse', 
        or 'inconclusive'. The isCausal field can only be either 'True' or 'False', and can only be true if the text directly states that the relationship is a causal relationship.
        The SupportText field of your output should include a section of verbatim from the text in addition to any comments you want to make about your output, without paraphrasing.
        Use exactly wording of outputs choices and input variable names, including capitalization choices.

        {format_instructions}
        """,
        input_variables=["text"],
        partial_variables={"format_instructions":parser.get_format_instructions}
    )
    input_text = prompt.format_prompt(text=processed_text).to_string()
    if verbose:
        with open(OUTPUTS_SOURCE + "\MultiVariablePipelineInput.txt", "a") as f:
            f.write("Input begins:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    if verbose:
        print("what is human_message_prompt:", type(human_message_prompt))
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    if verbose:
        print("what is chat_prompt:", type(chat_prompt))
    completion_prompt = chat_prompt.format_prompt(text=processed_text).to_messages()
    if verbose:
        print("What is a completion_prompt:", type(completion_prompt))

    # Create LLM
    model = ChatOpenAI(temperature=.3, openai_api_key=key, model_name=model)

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        print("what is a output:", type(output), output.content)
        with open(OUTPUTS_SOURCE + "\MultiVariablePipelineOutput.txt", "a") as f:
            f.write("pre parse: ")
            f.write(str(output.content))
            f.write("\n")
    parsed_output = parser.parse(output.content)
    return output

if __name__ == "__main__":
    # Prepare inputs:
    text = str()
    with open(PAPER_PATH) as f:
        text = f.read()

    # Process and save outputs:
    output = extract_relationships(text, verbose=True, model=MODEL_NAME)
    with open(OUTPUTS_SOURCE + "\MultiVariablePipelineOutput.txt", "a") as f:
        f.write("successful parse MULTIRELATION: ")
        f.write(str(output.content))
        f.write("\n")


========== END OF evaluations/pipelines/w_multi_paper_pipeline.py ==========

(Code File) evaluations/pipelines/multiple_choice_pipeline.py


========== START OF evaluations/pipelines/multiple_choice_pipeline.py ==========

import torch
import numpy as np
from transformers import LongformerTokenizer, LongformerForMultipleChoice

def prepare_answering_input(
        tokenizer, # longformer_tokenizer
        question,  # str
        options,   # List[str]
        context,   # str
        max_seq_length=4096,
    ):
    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question
    c_plus_q_4 = [c_plus_q] * len(options)
    tokenized_examples = tokenizer(
        c_plus_q_4, options,
        max_length=max_seq_length,
        padding="longest",
        truncation=True,
        return_tensors="pt",
    )
    input_ids = tokenized_examples['input_ids'].unsqueeze(0)
    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)
    example_encoded = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }
    return example_encoded

# What if we can make GRADCAM for LLMs that allows us to see whether something is the case.

tokenizer = LongformerTokenizer.from_pretrained("potsawee/longformer-large-4096-answering-race")
model = LongformerForMultipleChoice.from_pretrained("potsawee/longformer-large-4096-answering-race")

context = r"""Importantly, individuals assigned to the rumination condition received higher rumination scores than participants in the cognitive distraction condition in both theMDD, F(1, 40)=54.25, p<.001, η2=0.59, and CTL group, F(1, 44)=18.73, p<.001, η2=0.30.
""".replace("\n", "")
question = r"""Relationships between variables can only be 'direct', 'inverse', 'not applicable', or 'uncorrelated'.
  Direct means that an increase in one would cause a decrease in another, and inverse means that an increase in one would cause a decrease in another.
  Uncorrelated means there is no clear relationship between the two variables. Not applicable means the text does not suggest anything about the relationship between the two variables. 
  Determine what the text implies about the relationship between Cognitive distraction -> Rumination, namely if the relation is 'direct', 'inverse', 'not applicable', or 'uncorrelated'.
  """
options  = ['direct', 'inverse', 'not applicable', 'uncorrelated']

inputs = prepare_answering_input(tokenizer=tokenizer, options=options, question=question, context=context)
outputs = model(**inputs)
prob = torch.softmax(outputs.logits, dim=-1)[0].tolist()
selected_answer = options[np.argmax(prob)]

print(dict(zip(options,prob)))
print(selected_answer)

========== END OF evaluations/pipelines/multiple_choice_pipeline.py ==========

(Code File) evaluations/pipelines/captured_relations_pipeline.py


========== START OF evaluations/pipelines/captured_relations_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os
import json
from pprint import pprint
import yaml

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

# Filepath Debug
mypath = os.path.abspath("")
        
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    SupportingText: str
    Reasoning: str
    RelationshipClassification: str
    
    @validator("RelationshipClassification")
    def allowed_classifications(cls, field):
        if field.lower() in {"direct", "inverse", "not applicable", "uncorrelated"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

class ListOfRelations(BaseModel):
    Relations: list[SingleRelation]

def extract_relationships(data, set_prompt=None, verbose = False, model = None, verbatim=False, outputs_source=None):
    """
    12/19/2023 (Function Last Updated)
    Data should be a python dictionary cleaned of all ground truth data. 
    Verbose triggers storage of information into debug files, this may break depending on where you run this script from as it depends on relative paths.
    Model specifies LLM to be used. (This file was only tested with OpenAI LLMs)
    Verbatim feeds the exact formatting of the dictionary found in the ground truth encoding into the prompt. (Minus the ground truth data)
    Verbatim will make it hard for the model to decide whether or not to include fields that were not in ground truth if you modify the SingleRelation class.
    outputs_source, path to the file where verbose will dump debug information
    """
    # Add map reduce or some other type of summarization function here.
    processed_text = data["PaperContents"]
    if verbatim:
        relationships = {"Relations": data["Relations"]}
    else:
        relationships = extract_all_ordered_pairs(data)

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=ListOfRelations) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    prompt = PromptTemplate(
        template=set_prompt,
        input_variables=["text", "relationships"],
        partial_variables={"format_instructions":parser.get_format_instructions}
    )
    input_text = prompt.format_prompt(text=processed_text, relationships=relationships).to_string()
    if verbose:
        with open(outputs_source / "MultiVariablePipelineInput.txt", "a") as f:
            f.write("="*70+f"\nPaper Analyzed: {data['PaperTitle']}"+"\nLLM Prompt:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    completion_prompt = chat_prompt.format_prompt(text=processed_text, relationships=relationships).to_messages()

    # Create LLM
    model = ChatOpenAI(temperature=.0, openai_api_key=key, model_name=model)

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        with open(outputs_source / "MultiVariablePipelineOutput.txt", "a") as f:
            f.write("="*70+f"\nPaper Analyzed: {data['PaperTitle']}"+"\nLLM Pipeline Output:\n")
            f.write(str(output.content))
            f.write("\n")
    parsed_output = parser.parse(output.content) # Ensure content is in valid json format.
    if verbose:
        print(f"\n=========\nSuccessful pipeline completion for {data['PaperTitle'][:50]}, debug information and results saved at {outputs_source}")
    return parsed_output.dict()  # Returns in dict format

def clean_data(data_path, verbose=False) -> dict():
    """Reads Json and removes list of user predictions"""
    with open(data_path, "r") as f:
        data = json.load(f)
    for relation in data['Relations']:
        relation["RelationshipClassification"] = ""
        relation["IsCausal"] = ""
        relation["SupportingText"] = ""
        relation["Attributes"] = ""
    if verbose:
        pprint(data)
    return data  

def extract_all_ordered_pairs(data):
    #Extract the relationships
    relationships = data.get("Relations", [])
    variable_pairs = [f"Below are {len(relationships)} relations:"]
    # Iterate through each relationship and extract variables
    for relationship in relationships:
        variable_one = relationship.get("VariableOneName", "")
        variable_two = relationship.get("VariableTwoName", "")
        variable_pairs.append(variable_one + " -> " + variable_two)
    relations_text = "\n".join(variable_pairs)
    return relations_text

def make_unique_names(relations):
    """Takes a list of relations, and combines var one and var two names into the field UniqueName"""
    for relationship in relations:
        relationship["UniqueName"] = make_unique_name(relationship)

def make_unique_name(relationship):
    variable_one = relationship.get("VariableOneName", "")
    variable_two = relationship.get("VariableTwoName", "")
    return variable_one + " -> " + variable_two

def match_relation_to_paper():
    """Reference passage ranking section https://huggingface.co/tasks/sentence-similarity"""
    pass

def obtain_papers_via_MLSE():
    """Obtains relevant papers via the massive literature search engine"""
    pass

def captured_relations_pipeline(data_path, settings_path, debug_path: pathlib.Path):
    with open(settings_path, "r") as f:
        pipeline_settings = yaml.safe_load(f)
        verbose = pipeline_settings["verbose"]
        prompt = pipeline_settings["prompt"]
        model = pipeline_settings["model"]
    cleaned_data = clean_data(data_path, verbose=verbose)
    output = extract_relationships(cleaned_data, set_prompt=prompt, verbose=verbose, model=model, outputs_source=debug_path)
    return output

if __name__ == "__main__":
    """This section configs the run's default debug settings for running the file without the evaluator"""
    captured_relations_pipeline(data_path="../evaluation_datasets/multi_relation_dataset/test_paper.json",
                                settings_path="../pipeline_settings.yaml",
                                debug_path=pathlib.Path("./debug_outputs"))
    pass


========== END OF evaluations/pipelines/captured_relations_pipeline.py ==========

(Code File) evaluations/pipelines/multi_paper_pipeline.py


========== START OF evaluations/pipelines/multi_paper_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

# Filepath Debug
mypath = os.path.abspath("")
print("___\n\n\n", mypath)

# Configuration
"""This section configs the run"""
PAPER_SOURCE = pathlib.Path("../../papers")
OUTPUTS_SOURCE = pathlib.Path("../../outputs")
PAPER_FILENAME = "RuminationandCognitiveDistractionin_10.1007_s10862_015_9510_1.text"
MODEL_NAME = "gpt-3.5-turbo-1106"

#I am getting an error when running this in the slae_repo/SLAE-initial-testing-dataset on Windows
#I don't wanna edit/delete this code, so I will just make another file and then test out if that work for everyone else as well
#The changes in this fily only include minor changes (ai deletion of unneccessary variables, and putting already created constants to the model)

# Configure output parser classes
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    RelationshipClassification: str
    isCausal: str
    SupportingText: str

    @validator("RelationshipClassification")
    def question_ends_with_question_mark(cls, field):
        if field.lower() in {"direct", "inverse", "inconclusive"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

class ListOfRelations(BaseModel):
    Relations: list[SingleRelation]

def extract_relationships(text, verbose = False, model = MODEL_NAME):
    # Add map reduce or some other type of summarization function here.
    processed_text = text

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=ListOfRelations) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    formatting_text = """
    The output should be formatted as a JSON instance that conforms to the JSON schema below.

    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

    Here is the output schema:
    ```
    {"Relationships":[{"properties": {"VariableOneName": {"title": "Variableonename", "type": "string"}, "VariableTwoName": {"title": "Variabletwoname", "type": "string"}, "RelationshipClassification": {"title": "Relationshipclassification", "type": "string"}, "isCausal": {"title": "Iscausal", "type": "string"}, "SupportingText": {"title": "Supportingtext", "type": "string"}}, "required": ["VariableOneName", "VariableTwoName", "RelationshipClassification", "isCausal", "SupportingText"]}]}
    ```"""
    prompt = PromptTemplate(
        template="""
        {text}

        Given the text identify a series of relationships between variables that have a well defined positive direction and a clear textual backing.
        For example, if a text says "We find a strong correlation p < .0001 between a country's per capita income and it's literacy rate" then "Country's per capita income" and "Country's literacy rate" would be good variables. Variables that are implied but not directly stated by the text such as "education infilstructure" should not be included. 
        The RelationshipClassification field can only be 'direct', 'inverse', 
        or 'inconclusive'. The isCausal field can only be either 'True' or 'False', and can only be true if the text directly states that the relationship is a causal relationship.
        The SupportText field of your output should include a section of verbatim from the text in addition to any comments you want to make about your output, without paraphrasing.
        Use exactly wording of outputs choices and input variable names, including capitalization choices.

        {format_instructions}
        """,
        input_variables=["text"],
        partial_variables={"format_instructions":parser.get_format_instructions}
    )
    input_text = prompt.format_prompt(text=processed_text).to_string()
    if verbose:
        with open(OUTPUTS_SOURCE / "MultiVariablePipelineInput.txt", "a") as f:
            f.write("Input begins:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    if verbose:
        print("what is human_message_prompt:", type(human_message_prompt))
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    if verbose:
        print("what is chat_prompt:", type(chat_prompt))
    completion_prompt = chat_prompt.format_prompt(text=processed_text).to_messages()
    if verbose:
        print("What is a completion_prompt:", type(completion_prompt))

    # Create LLM
    model = ChatOpenAI(temperature=.3, openai_api_key=key, model_name=model)

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        print("what is a output:", type(output), output.content)
        with open(OUTPUTS_SOURCE / "MultiVariablePipelineOutput.txt", "a") as f:
            f.write("pre parse: ")
            f.write(str(output.content))
            f.write("\n")
    parsed_output = parser.parse(output.content)
    return output

if __name__ == "__main__":
    # Prepare inputs:
    text = str()
    with open(PAPER_SOURCE / "RuminationandCognitiveDistractionin_10.1007_s10862_015_9510_1.txt") as f:
        text = f.read()

    # Process and save outputs:
    output = extract_relationships(text, verbose=True, model=MODEL_NAME)
    with open(OUTPUTS_SOURCE / "MultiVariablePipelineOutput.txt", "a") as f:
        f.write("successful parse MULTIRELATION: ")
        f.write(str(output.content))
        f.write("\n")

========== END OF evaluations/pipelines/multi_paper_pipeline.py ==========

(Code File) evaluations/pipelines/run_swag.py


========== START OF evaluations/pipelines/run_swag.py ==========

#!/usr/bin/env python
# coding=utf-8
# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Fine-tuning the library models for multiple choice.
"""
# You can also adapt this script on your own multiple choice task. Pointers for this are left as comments.

import logging
import os
import sys
from dataclasses import dataclass, field
from itertools import chain
from typing import Optional, Union

import datasets
import numpy as np
import torch
import transformers
from datasets import load_dataset
from transformers import (
    AutoConfig,
    AutoModelForMultipleChoice,
    AutoTokenizer,
    HfArgumentParser,
    default_data_collator,
    set_seed,
)
from transformers.tokenization_utils_base import PreTrainedTokenizerBase
from transformers.trainer_utils import get_last_checkpoint
from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry

from optimum.graphcore import IPUConfig, IPUTrainer
from optimum.graphcore import IPUTrainingArguments as TrainingArguments
from optimum.graphcore.utils import check_min_version as gc_check_min_version


# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
check_min_version("4.29.0")

# Will error if the minimal version of Optimum Graphcore is not installed. Remove at your own risks.
gc_check_min_version("0.6.0.dev0")

logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )
    use_fast_tokenizer: bool = field(
        default=True,
        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
    )
    model_revision: str = field(
        default="main",
        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
    )
    use_auth_token: bool = field(
        default=False,
        metadata={
            "help": (
                "Will use the token generated when running `huggingface-cli login` (necessary to use this script "
                "with private models)."
            )
        },
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    train_file: Optional[str] = field(default=None, metadata={"help": "The input training data file (a text file)."})
    validation_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input evaluation data file to evaluate the perplexity on (a text file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    max_seq_length: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "The maximum total input sequence length after tokenization. If passed, sequences longer "
                "than this will be truncated, sequences shorter will be padded."
            )
        },
    )
    pad_to_max_length: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to pad all samples to the maximum sentence length. "
                "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
                "efficient on GPU but very bad for TPU."
            )
        },
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )

    def __post_init__(self):
        if self.train_file is not None:
            extension = self.train_file.split(".")[-1]
            assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
        if self.validation_file is not None:
            extension = self.validation_file.split(".")[-1]
            assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."


@dataclass
class DataCollatorForMultipleChoice:
    """
    Data collator that will dynamically pad the inputs for multiple choice received.

    Args:
        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):
            The tokenizer used for encoding the data.
        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):
            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
            among:

            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence
              if provided).
            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum
              acceptable input length for the model if that argument is not provided.
            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different
              lengths).
        max_length (`int`, *optional*):
            Maximum length of the returned list and optionally padding length (see above).
        pad_to_multiple_of (`int`, *optional*):
            If set will pad the sequence to a multiple of the provided value.

            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
            7.5 (Volta).
    """

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0]["input_ids"])
        flattened_features = [
            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features
        ]
        flattened_features = list(chain(*flattened_features))

        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        # Un-flatten
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        batch["labels"] = torch.tensor(labels, dtype=torch.int64)
        return batch


def main():
    # See all possible arguments in src/transformers/training_args.py
    # or by passing the --help flag to this script.
    # We now keep distinct sets of args, for a cleaner separation of concerns.

    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments.
        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The
    # information sent is the one passed as arguments along with your Python/PyTorch versions.
    send_example_telemetry("run_swag", model_args, data_args)

    # Setup logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    if training_args.should_log:
        # The default of training_args.log_level is passive, so we set log level at info here to have that default.
        transformers.utils.logging.set_verbosity_info()

    log_level = training_args.get_process_log_level()
    logger.setLevel(log_level)
    datasets.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.set_verbosity(log_level)
    transformers.utils.logging.enable_default_handler()
    transformers.utils.logging.enable_explicit_format()

    logger.info(f"Training/evaluation parameters {training_args}")

    # Detecting last checkpoint.
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
            raise ValueError(
                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
                "Use --overwrite_output_dir to overcome."
            )
        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:
            logger.info(
                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
            )

    # Set seed before initializing model.
    set_seed(training_args.seed)

    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)
    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/
    # (the dataset will be downloaded automatically from the datasets Hub).

    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called
    # 'text' is found. You can easily tweak this behavior (see below).

    # In distributed training, the load_dataset function guarantee that only one local process can concurrently
    # download the dataset.
    if data_args.train_file is not None or data_args.validation_file is not None:
        data_files = {}
        if data_args.train_file is not None:
            data_files["train"] = data_args.train_file
        if data_args.validation_file is not None:
            data_files["validation"] = data_args.validation_file
        extension = data_args.train_file.split(".")[-1]
        raw_datasets = load_dataset(
            extension,
            data_files=data_files,
            cache_dir=model_args.cache_dir,
            use_auth_token=True if model_args.use_auth_token else None,
        )
    else:
        # Downloading and loading the swag dataset from the hub.
        raw_datasets = load_dataset(
            "swag",
            "regular",
            cache_dir=model_args.cache_dir,
            use_auth_token=True if model_args.use_auth_token else None,
        )
    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at
    # https://huggingface.co/docs/datasets/loading_datasets.html.

    # Load pretrained model and tokenizer

    # Distributed training:
    # The .from_pretrained methods guarantee that only one local process can concurrently
    # download model & vocab.
    config = AutoConfig.from_pretrained(
        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    ipu_config = IPUConfig.from_pretrained(
        training_args.ipu_config_name if training_args.ipu_config_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
        cache_dir=model_args.cache_dir,
        use_fast=model_args.use_fast_tokenizer,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )
    model = AutoModelForMultipleChoice.from_pretrained(
        model_args.model_name_or_path,
        from_tf=bool(".ckpt" in model_args.model_name_or_path),
        config=config,
        cache_dir=model_args.cache_dir,
        revision=model_args.model_revision,
        use_auth_token=True if model_args.use_auth_token else None,
    )

    # When using your own dataset or a different dataset from swag, you will probably need to change this.
    ending_names = [f"ending{i}" for i in range(4)]
    context_name = "sent1"
    question_header_name = "sent2"

    if data_args.max_seq_length is None:
        max_seq_length = tokenizer.model_max_length
        if max_seq_length > 1024:
            logger.warning(
                "The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value"
                " of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can"
                " override this default with `--block_size xxx`."
            )
            max_seq_length = 1024
    else:
        if data_args.max_seq_length > tokenizer.model_max_length:
            logger.warning(
                f"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the"
                f"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}."
            )
        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)

    # Preprocessing the datasets.
    def preprocess_function(examples):
        first_sentences = [[context] * 4 for context in examples[context_name]]
        question_headers = examples[question_header_name]
        second_sentences = [
            [f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)
        ]

        # Flatten out
        first_sentences = list(chain(*first_sentences))
        second_sentences = list(chain(*second_sentences))

        # Tokenize
        tokenized_examples = tokenizer(
            first_sentences,
            second_sentences,
            truncation=True,
            max_length=max_seq_length,
            padding="max_length" if data_args.pad_to_max_length else False,
        )
        # Un-flatten
        return {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}

    if training_args.do_train:
        if "train" not in raw_datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = raw_datasets["train"]
        if data_args.max_train_samples is not None:
            max_train_samples = min(len(train_dataset), data_args.max_train_samples)
            train_dataset = train_dataset.select(range(max_train_samples))
        with training_args.main_process_first(desc="train dataset map pre-processing"):
            train_dataset = train_dataset.map(
                preprocess_function,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                load_from_cache_file=not data_args.overwrite_cache,
            )

    if training_args.do_eval:
        if "validation" not in raw_datasets:
            raise ValueError("--do_eval requires a validation dataset")
        eval_dataset = raw_datasets["validation"]
        if data_args.max_eval_samples is not None:
            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
            eval_dataset = eval_dataset.select(range(max_eval_samples))
        with training_args.main_process_first(desc="validation dataset map pre-processing"):
            eval_dataset = eval_dataset.map(
                preprocess_function,
                batched=True,
                num_proc=data_args.preprocessing_num_workers,
                load_from_cache_file=not data_args.overwrite_cache,
            )

    # Data collator
    data_collator = (
        default_data_collator
        if data_args.pad_to_max_length
        else DataCollatorForMultipleChoice(tokenizer=tokenizer, pad_to_multiple_of=None)
    )

    if not data_args.pad_to_max_length:
        logging.warning(
            "Not padding to max length might lead to batches with difference sequence lengths, which might not work as"
            "expected on IPUs"
        )

    # Metric
    def compute_metrics(eval_predictions):
        predictions, label_ids = eval_predictions
        preds = np.argmax(predictions, axis=1)
        return {"accuracy": (preds == label_ids).astype(np.float32).mean().item()}

    # Initialize our Trainer
    trainer = IPUTrainer(
        model=model,
        ipu_config=ipu_config,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Training
    if training_args.do_train:
        checkpoint = None
        if training_args.resume_from_checkpoint is not None:
            checkpoint = training_args.resume_from_checkpoint
        elif last_checkpoint is not None:
            checkpoint = last_checkpoint
        train_result = trainer.train(resume_from_checkpoint=checkpoint)
        trainer.save_model()  # Saves the tokenizer too for easy upload
        metrics = train_result.metrics

        max_train_samples = (
            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
        )
        metrics["train_samples"] = min(max_train_samples, len(train_dataset))

        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()

    # Evaluation
    if training_args.do_eval:
        logger.info("*** Evaluate ***")

        metrics = trainer.evaluate()
        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))

        trainer.log_metrics("eval", metrics)
        trainer.save_metrics("eval", metrics)

    kwargs = {
        "finetuned_from": model_args.model_name_or_path,
        "tasks": "multiple-choice",
        "dataset_tags": "swag",
        "dataset_args": "regular",
        "dataset": "SWAG",
        "language": "en",
    }

    if training_args.push_to_hub:
        trainer.push_to_hub(**kwargs)
    else:
        trainer.create_model_card(**kwargs)


def _mp_fn(index):
    # For xla_spawn (TPUs)
    main()


if __name__ == "__main__":
    main()


========== END OF evaluations/pipelines/run_swag.py ==========

(Code File) evaluations/pipelines/single_variable_pipeline.py


========== START OF evaluations/pipelines/single_variable_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

# Filepath Debug
mypath = os.path.abspath("")
print("___\n\n\n", mypath)

# Configuration
"""This section configs the run"""
PAPER_SOURCE = pathlib.Path("../../papers")
OUTPUTS_SOURCE = pathlib.Path("../../outputs")
PAPER_FILENAME = "testpaper.txt"
MODEL_NAME = "gpt-3.5-turbo-1106"

# Configure output parser classes
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    RelationshipClassification: str
    isCausal: str
    SupportingText: str

    @validator("RelationshipClassification")
    def question_ends_with_question_mark(cls, field):
        if field.lower() in {"direct", "inverse", "inconclusive"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

def extract_relationships(text, variable_one, variable_two, verbose = False, model = MODEL_NAME):
    # Add map reduce or some other type of summarization function here.
    processed_text = text

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=SingleRelation) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    prompt = PromptTemplate(
        template = """
        {text}
        
        Given the text, identify the relationship between {variable_one} 
        and {variable_two}.
        
        {format_instructions}

        The RelationshipClassification field can only be 'direct', 'inverse', 
        or 'inconclusive'. The isCausal field can only be either 'True' or 'False', and can only be 
        true if the text directly states that the relationship is a causal relationship.
        The SupportText field of your output should include a section 
        of verbatim from the text in addition to any comments you want to make about your output.
        Use exactly wording of outputs choices and input variable names, including capitalization.""",
        input_variables=["text", "variable_one", "variable_two"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    input_text = prompt.format_prompt(text=processed_text, variable_one=variable_one, variable_two=variable_two).to_string()
    if verbose:
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineInput.txt", "a") as f:
            f.write("Input begins:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    if verbose:
        print("what is human_message_prompt:", type(human_message_prompt))
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    if verbose:
        print("what is chat_prompt:", type(chat_prompt))
    completion_prompt = chat_prompt.format_prompt(text=processed_text, variable_one=variable_one, variable_two=variable_two).to_messages()
    if verbose:
        print("What is a completion_prompt:", type(completion_prompt))

    # Create LLM
    model = ChatOpenAI(temperature=.3, openai_api_key=key, model_name=model)

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        print("what is a output:", type(output), output.content)
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
            f.write("pre parse: ")
            f.write(str(output.content))
            f.write("\n")
    output = parser.parse(output.content)
    return output

if __name__ == "__main__":
    # Prepare inputs:
    text = str()
    with open(PAPER_SOURCE / PAPER_FILENAME) as f:
        text = f.read()
    variable_one = "AI/AN status"
    variable_two = "Substance use"

    # Process and save outputs:
    output = extract_relationships(text, variable_one, variable_two, verbose=True)
    with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
        f.write("successful parse: ")
        f.write(str(type(output.json()))+output.json())
        f.write("\n")

========== END OF evaluations/pipelines/single_variable_pipeline.py ==========

(Directory) evaluations/auto_generated_inputs
(Code File) evaluations/single_relation_evaluator.py


========== START OF evaluations/single_relation_evaluator.py ==========

"""
General purpose evaluator should take in a YAML file which specifies the parameters: type of model, evaluation dataset, type of scoring function, and supporting tools combination
Run the appropriate pipeline with these parameters.
Calculate the score.
Save results, score, predictions(inputs and outputs), other relevant information, in evaluation outputs.
"""
import pathlib
from pipelines.single_variable_pipeline import extract_relationships
import json
import os

mypath = os.path.abspath("")
print("___\n\n\n", mypath)

# Read a YAML file
dataset_path = pathlib.Path("evaluation_datasets/single_relation_dataset")

# Read evaluation dataset
with open(dataset_path / "1.json") as f:
    ground_truth = json.load(f)
    print("datatype of ground truth", type(ground_truth))

# Determine Settings
text = ground_truth["text"]
VariableOneName = ground_truth["VariableOneName"]
VariableTwoName = ground_truth["VariableTwoName"]
RelationshipClassification = ground_truth["RelationshipClassification"]
isCausal = ground_truth["isCausal"]

# extract_relationships()
# prediction = extract_relationships(text, VariableOneName, VariableTwoName).dict()
prediction = ground_truth.copy()
prediction["RelationshipClassification"] = "inverse"
print("datatype of prediction", type(prediction))
# compare to obtain score
def compare(prediction, ground_truth):
    score_dictionary = dict()
    if prediction["RelationshipClassification"] == ground_truth["RelationshipClassification"]:
        score_dictionary["RelationshipClassificationScore"] = 1
        print("RelationshipClassification Score ==> success")
    else:
        score_dictionary["RelationshipClassificationScore"] = 0
        print("RelationshipClassification Score ==> actual:", ground_truth["RelationshipClassification"], "; predicted:", prediction["RelationshipClassification"])
    
    if prediction["isCausal"] == ground_truth["isCausal"]:
        score_dictionary["isCausalScore"] = 1
        print("Causal Score ==> success")
    else:
        score_dictionary["isCausalScore"] = 0
        print("Causal Score ==> actual:", ground_truth["isCausal"], "; predicted:", prediction["isCausal"])

compare(prediction, ground_truth)

========== END OF evaluations/single_relation_evaluator.py ==========

(Directory) evaluations/__pycache__
(Directory) evaluations/matcher
(Code File) evaluations/matcher/BM25.py


========== START OF evaluations/matcher/BM25.py ==========

import pandas as pd
from rank_bm25 import *
from string import punctuation
import nltk
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import PorterStemmer
from nltk.corpus import PlaintextCorpusReader


stop_words = STOPWORDS.union(set(['the','of','and','in','to','a','an','was','as','by','for','is','on','at','onn','that','with']))
def remove_stopwords(lst):
    tokenized_cleaned = list()
    for str in lst:
        tokens = nltk.word_tokenize(str)
        tokens_cleaned = [word for word in tokens if not word.lower() in stop_words and not word in punctuation]
        str_cleaned = " ".join(tokens_cleaned)
        tokenized_cleaned.append(str_cleaned)

def get_scores(folder_path, query):
    file_pattern = r'.*\.txt'
    corpus = PlaintextCorpusReader(folder_path, file_pattern)

    documents = [corpus.raw(file_id) for file_id in corpus.fileids()]
    lst1 =  remove_stopwords(documents)
    tokenized_corpus = [doc.split(" ") for doc in lst1]
    bm25 = BM25Okapi(tokenized_corpus)
    tokenized_query=query.split(" ")
    return bm25.get_scores(tokenized_query)


========== END OF evaluations/matcher/BM25.py ==========

(Code File) evaluations/matcher/matcher.py


========== START OF evaluations/matcher/matcher.py ==========

import os
import json
from pprint import pprint
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Initialize the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def compute_similarity(embedding1, embedding2):
    # Cosine similarity
    return np.dot(embedding1, embedding2.T) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))

def passage_rank(relation, papers, threshold=0.25):
    # Tokenize and encode the relation
    inputs = tokenizer(relation, return_tensors="pt", padding=True, truncation=True, max_length=512)
    relation_embedding = model(**inputs).last_hidden_state.mean(dim=1).detach().numpy()
    
    relevant_papers = []
    for paper in papers:
        # Tokenize and encode each paper
        paper_inputs = tokenizer(paper["PaperContents"], return_tensors="pt", padding=True, truncation=True, max_length=512)
        # paper inputs are a dictionary consisting of the embeddings, type, and attention mask.
        paper_embedding = model(**paper_inputs).last_hidden_state.mean(dim=1).detach().numpy()
        
        # Compute similarity
        similarity = compute_similarity(relation_embedding, paper_embedding)
        print(paper["PaperTitle"][:50], relation, similarity)
        if similarity > threshold:
            relevant_papers.append(paper)
    return relevant_papers


def match_relations_to_papers(papers_directory = "../evaluations/auto_generated_inputs", input_relations_directory = "../evaluations/test_inputs/test_input.json", debug = False):
    papers = []

    for file in os.listdir(papers_directory):
        input_path = os.path.join(papers_directory, file)
        with open(input_path, "r") as f:
            data = json.load(f)
            papers.append(data)
            # print(input_path)


    input_path = input_relations_directory
    with open(input_path, "r") as f:
        data = json.load(f)
    relationships = data["Relations"]

    variable_pairs = []
    # Iterate through each relationship and extract variables
    for relationship in relationships:
        variable_one = relationship.get("VariableOneName", "")
        variable_two = relationship.get("VariableTwoName", "")
        variable_pairs.append(variable_one + " -> " + variable_two)

    print(variable_pairs)

    relations_and_ranked_papers: dict = dict()
    for i in range(len(variable_pairs)):
        relation = variable_pairs[i]
        relevant_papers = passage_rank(relation, papers) # relevant papers is a list of relevant paper dictionaries with relevant metadata
        relations_and_ranked_papers[i] = {j["PaperDOI"] for j in relevant_papers}

    print(relations_and_ranked_papers)

    # Goal: something in the form of list of paper objects with relations
    for i in range(len(papers)):   
        papers[i]["Relations"] = []
        for key in relations_and_ranked_papers:
            value = relations_and_ranked_papers[key]
            if papers[i]["PaperDOI"] in value:
                # print(relationships[key])
                papers[i]["Relations"].append(relationships[key])

        print("Paper titled", papers[i]["PaperTitle"][:50], "Matched with", len(papers[i]["Relations"]), "relations.")


    if debug:
        with open("../evaluations/test_inputs/output.json", "w") as f:
            data_out = json.dumps({"papers":papers})
            f.write(data_out)
            pprint({"papers":papers})

    return papers

========== END OF evaluations/matcher/matcher.py ==========

(Code File) evaluations/matcher/embedding.py


========== START OF evaluations/matcher/embedding.py ==========

import gensim
import nltk
from gensim.models import word2vec, Phrases, Word2Vec
from gensim.models.phrases import Phraser
from string import punctuation
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.util import ngrams


def embedding(text, search_words):

    stop_words = set(stopwords.words('english'))
    stop_words.update(['the','of','and','in','to','a','an','was','as','by','for','is','on','at','onn','that','with'])
    token = nltk.word_tokenize(text)
    token_cleaned = [word for word in token if word.lower() not in stop_words and word not in punctuation]

    text_cleaned = ' '.join(token_cleaned)

    sentences = nltk.sent_tokenize(text_cleaned)
    bigram =Phraser(Phrases(sentences, min_count=1))
    trigram = Phraser(Phrases(bigram[sentences],min_count=1))

    trigram_sentences_project = []
    for sent in sentences:
        bigrams_ = bigram[sent.split(" ")]
        trigrams_ = trigram[bigram[sent.split(" ")]]
        trigram_sentences_project.append(trigrams_)

    # Defining parameters
    num_features = 200 # Word Vector Dimensionality (How many number in one vector?)
    min_word_count = 1 # Minimum word count
    num_workers = 20 # Number of threads to run in parallel
    context = 5 #Context window size
    downsampling = 1e-3 # Downsample setting for frequent word

    model = word2vec.Word2Vec(trigram_sentences_project,
                              workers=num_workers,
                              vector_size=num_features,
                              min_count=min_word_count,
                              window=context,
                              sample=downsampling)
    if isinstance(search_words, list):
        for word in search_words:
            print(f"Most similar words to '{word}':")
            print(model.wv.most_similar(word, topn=10))
    elif isinstance(search_words, str):
        print(f"Most similar words to '{search_words}':")
        print(model.wv.most_similar(search_words, topn=10))

        


def cosine_similarity(text, word1, word2):

    stop_words = set(stopwords.words('english'))
    stop_words.update(['the','of','and','in','to','a','an','was','as','by','for','is','on','at','onn','that','with'])
    token = nltk.word_tokenize(text)
    token_cleaned = [word for word in token if word.lower() not in stop_words and word not in punctuation]

    text_cleaned = ' '.join(token_cleaned)

    sentences = nltk.sent_tokenize(text_cleaned)
    bigram =Phraser(Phrases(sentences, min_count=1))
    trigram = Phraser(Phrases(bigram[sentences],min_count=1))

    trigram_sentences_project = []
    for sent in sentences:
        bigrams_ = bigram[sent.split(" ")]
        trigrams_ = trigram[bigram[sent.split(" ")]]
        trigram_sentences_project.append(trigrams_)

    # Defining parameters
    num_features = 200 # Word Vector Dimensionality (How many number in one vector?)
    min_word_count = 1 # Minimum word count
    num_workers = 20 # Number of threads to run in parallel
    context = 5 #Context window size
    downsampling = 1e-3 # Downsample setting for frequent word

    model = word2vec.Word2Vec(trigram_sentences_project,
                              workers=num_workers,
                              vector_size=num_features,
                              min_count=min_word_count,
                              window=context,
                              sample=downsampling)
    return f"The score between {word1} and {word2} is {model.wv.relative_cosine_similarity(word1, word2)}"

def embedding_stemmed(text, search_words):
    stop_words = set(stopwords.words('english'))
    stop_words.update(['the','of','and','in','to','a','an','was','as','by','for','is','on','at','onn','that','with'])
    porter = PorterStemmer()

    
    token = nltk.word_tokenize(text)

    token_cleaned_stemmed = [porter.stem(word) for word in token if word.lower() not in stop_words and word not in punctuation]

    text_stemmed = ' '.join(token_cleaned_stemmed)

    sentences = nltk.sent_tokenize(text_stemmed)
    bigram =Phraser(Phrases(sentences, min_count=1))
    trigram = Phraser(Phrases(bigram[sentences],min_count=1))

    trigram_sentences_project = []
    for sent in sentences:
        bigrams_ = bigram[sent.split(" ")]
        trigrams_ = trigram[bigram[sent.split(" ")]]
        trigram_sentences_project.append(trigrams_)

    # Defining parameters
    num_features = 200 # Word Vector Dimensionality (How many number in one vector?)
    min_word_count = 1 # Minimum word count
    num_workers = 20 # Number of threads to run in parallel
    context = 5 #Context window size
    downsampling = 1e-3 # Downsample setting for frequent word

    model = word2vec.Word2Vec(trigram_sentences_project,
                              workers=num_workers,
                              vector_size=num_features,
                              min_count=min_word_count,
                              window=context,
                              sample=downsampling)
    if isinstance(search_words, list):
        for word in search_words:
            print(f"Most similar words to '{word}', stemmed form:{porter.stem(word)}:")
            print(model.wv.most_similar(porter.stem(word), topn=10))
    elif isinstance(search_words, str):
        print(f"Most similar words to '{search_words}', stemmed form:{porter.stem(search_words)}:")
        print(model.wv.most_similar(porter.stem(search_words), topn=10))

========== END OF evaluations/matcher/embedding.py ==========

(Code File) evaluations/matcher/tf_idf.py


========== START OF evaluations/matcher/tf_idf.py ==========

import nltk
from nltk import word_tokenize
from gensim.corpora import Dictionary
from gensim.models import TfidfModel
from nltk.corpus import PlaintextCorpusReader

def tfidf(folder_path):
    file_pattern = r'.*\.txt'
    corpus = PlaintextCorpusReader(folder_path, file_pattern)

    documents = [corpus.raw(file_id) for file_id in corpus.fileids()]
    tokens = [word_tokenize(doc.lower()) for doc in documents]
    dictionary = Dictionary(tokens)
    corpus_bow = [dictionary.doc2bow(doc) for doc in tokens]

    return TfidfModel(corpus_bow)

# weights = tfidf[corpus_bow[0]]

# weights = [(dictionary[pair[0]], pair[1]) for pair in weights]

# pprint(sorted(weights, key=lambda weights: weights[1], reverse=True)[1:50])

========== END OF evaluations/matcher/tf_idf.py ==========

(Code File) evaluations/matcher/__init__.py


========== START OF evaluations/matcher/__init__.py ==========



========== END OF evaluations/matcher/__init__.py ==========

(Directory) evaluations/matcher/__pycache__
(Code File) evaluations/matcher/spacy_pretrained.py


========== START OF evaluations/matcher/spacy_pretrained.py ==========

import spacy
nlp = spacy.load('en_core_web_lg')

def similar_words(text, word):
    doc = nlp(text)
    seed_word = nlp(word)

    similar_words=[word.text for word in doc if word.similarity(seed_word) > 0.7]
    return similar_words

========== END OF evaluations/matcher/spacy_pretrained.py ==========

(Code File) evaluations/flow_control.py


========== START OF evaluations/flow_control.py ==========

import yaml
import pathlib
import json
from matcher.matcher import match_relations_to_papers
from pipelines.captured_relations_pipeline import extract_relationships
from pipeline_parser.parser import pipeline_to_kumu
from kumu_to_pipeline.parser import kumu_to_pipeline


#Debug
if False:
    with open(pathlib.Path("./test_inputs/test_input.json"), "r") as f:
        to_kumu_dict = json.load(f)
    pipeline_to_kumu(to_kumu_dict, "./test_inputs/test_input_from_kumu.json")


# Obtain list of relations from Kumu or Vensim file
if False:
    with open(pathlib.Path("./test_inputs/test_input.json"), "r") as f:
        kumu_read = kumu_to_pipeline(f)
    print(kumu_read)


# Use the matcher to separate the list of relations, resulting in a series of python dictionaries each with a single paper and a list of related relations.
matched_papers = match_relations_to_papers(papers_directory="./auto_generated_inputs", input_relations_directory="./test_inputs/test_input.json")
# print(matched_papers)


# Obtain settings by reading in the file
with open("./pipeline_settings.yaml", "r") as f:
    pipeline_settings = yaml.safe_load(f)
    verbose = pipeline_settings["verbose"]
    prompt = pipeline_settings["prompt"]
    model = pipeline_settings["model"]


# For each dictionary run the "extract_relationships" function and settings
# outputs = list()
# for data in matched_papers:
#     if len(data["Relations"]) == 0: 
#         print("\nPaper", data["PaperTitle"][:50], ": no matching relations") 
#         continue
#     else:
#         print("\nParsing", len(data["Relations"]), "relations from", data["PaperTitle"][:50])
#     output = extract_relationships(data, set_prompt=prompt, verbose=verbose, model=model, outputs_source=pathlib.Path("./pipelines/debug_outputs"))
#     output["PaperTitle"] = data["PaperTitle"]
#     output["PaperDOI"] = data["PaperDOI"]
#     # print(output["PaperDOI"])
#     outputs.append(output)
#     # print(outputs)
# outputs_dict = {"Papers" : outputs}
# debug = True
# if debug:
#     with open(pathlib.Path("./pipelines/debug_outputs") / "MultiVariablePipelineOutputs.json", "w") as f:
#         f.write(json.dumps(outputs_dict))
with open(pathlib.Path("./pipelines/debug_outputs") / "MultiVariablePipelineOutputs.json", "r") as f:
    outputs_dict = json.load(f)

# Obtain the list of output jsons from the iterative running of extract_relationships and recombine the output jsons into the format required to output to Kumu
pipeline_to_kumu(outputs_dict, "./pipelines/debug_outputs/")

# Parse it into Kumu form and save the json in a file. 

========== END OF evaluations/flow_control.py ==========

(Code File) evaluations/captured_relations_evaluator.py


========== START OF evaluations/captured_relations_evaluator.py ==========

"""
General purpose evaluator should take in a YAML file which specifies the parameters: type of model, evaluation dataset, type of scoring function, and supporting tools combination
Run the appropriate pipeline with these parameters.
Calculate the score.
Save results, score, predictions(inputs and outputs), other relevant information, in evaluation outputs.
"""
import pathlib
from pipelines.captured_relations_pipeline import captured_relations_pipeline
import json
import os
from copy import deepcopy

mypath = os.path.abspath("")
print("___\n\n\n", mypath)

def compare(prediction, ground_truth):
    score_dictionary = dict()
    if prediction["RelationshipClassification"].lower() == ground_truth["RelationshipClassification"].lower():
        score_dictionary["RelationshipClassificationScore"] = 1
        print("RelationshipClassification Score ==> success;", ground_truth["RelationshipClassification"])
    else:
        score_dictionary["RelationshipClassificationScore"] = 0
        print("RelationshipClassification Score ==> actual:", ground_truth["RelationshipClassification"], "; predicted:", prediction["RelationshipClassification"])
    
    # if prediction["IsCausal"].lower() == ground_truth["IsCausal"].lower():
    #     score_dictionary["IsCausalScore"] = 1
    #     print("Causal Score ==> success; ", ground_truth["IsCausal"])
    # else:
    #     score_dictionary["IsCausalScore"] = 0
    #     print("Causal Score ==> actual:", ground_truth["IsCausal"], "; predicted:", prediction["IsCausal"])
    return score_dictionary

def evaluate_one_paper(input_file_path, settings_path, strict_length=True, verbose=False, debug_path=None):
    # Read evaluation dataset
    with open(input_file_path) as f:
        ground_truth = json.load(f)
        print("datatype of ground truth", type(ground_truth))    

    # extract_relationships based on settings (which is text and nothing else)
    if True:
        predictions = captured_relations_pipeline(input_file_path, settings_path=settings_path, debug_path=debug_path)
    else:
        predictions = deepcopy(ground_truth)
        # Change predictions for testing
        predictions["PaperContents"] = ""
        predictions["Relations"][0]["RelationshipClassification"] = "inverse"
        print("datatype of prediction", type(predictions))

    # strip full text from ground truth to prevent accidental printing of full text
    ground_truth["PaperContents"] = ""

    # compare to obtain score
    if len(predictions["Relations"]) > len(ground_truth["Relations"]):
        index_max = len(ground_truth["Relations"])
        if strict_length:
            raise Exception("Prediction and Groundtruth lengths do not match")
    elif len(predictions["Relations"]) < len(ground_truth["Relations"]): 
        index_max = len(predictions["Relations"])
        if strict_length:
            raise Exception("Prediction and Groundtruth lengths do not match")
    else:
        index_max = len(predictions["Relations"])
        print("Number of relations in ground truth and predictions match.")

    results: list(dict()) = list()
    for relation_index in range(index_max):
        relation = ground_truth["Relations"][relation_index]
        prediction = predictions["Relations"][relation_index]
        results.append(compare(prediction, relation))

    aggregate_results = dict()
    aggregate_results["RelationshipClassificationScore"] = 0
    # aggregate_results["CausalIdentificationScore"] = 0
    for x, result in enumerate(results):
        aggregate_results["RelationshipClassificationScore"] += result["RelationshipClassificationScore"]
        # aggregate_results["CausalIdentificationScore"] += result["IsCausalScore"]
    aggregate_results["RelationshipClassificationScore"] /= len(results)
    # aggregate_results["CausalIdentificationScore"] /= len(results)
    print(aggregate_results)

    with open("evaluation_outputs/captured_relations_results/results.txt", "a+") as f:
        f.write(f"\nResults for {input_file_path.name}\n")
        f.write(json.dumps(aggregate_results, indent=2))
        f.write("\n")
        f.write("Predictions\n")
        f.write(json.dumps(predictions, indent=2))
        f.write("\n")
        f.write("Ground Truth\n")
        f.write(json.dumps(ground_truth, indent=2))
        f.write("\n")
    aggregate_results["file"] = input_file_path.name
    return aggregate_results

# Read a YAML file to obtain settings
DATASET_PATH = pathlib.Path("evaluation_datasets/multi_relation_dataset")
SETTINGS_PATH = pathlib.Path("./pipeline_settings.yaml")
DEBUG_PATH = pathlib.Path("evaluation_outputs/captured_relations_results/debug_outputs")
MULTIPAPER = True

if MULTIPAPER:
    with open("evaluation_outputs/captured_relations_results/results.txt", "w") as f:
        f.write(f"New multi file evaluation source from path: {DATASET_PATH}")
    dir, _, files = next(os.walk(DATASET_PATH))
    full_evaluator_aggregate_results = []
    for file in files: 
        print("\n\nEvaluating: ", file)
        result = evaluate_one_paper(pathlib.Path(dir)/pathlib.Path(file), settings_path=SETTINGS_PATH, verbose=True, debug_path=DEBUG_PATH)
        full_evaluator_aggregate_results.append(result)
    with open("evaluation_outputs/captured_relations_results/results.txt", "a+") as f:
        f.write("\n\nAggregated_Results:\n")
        for i in full_evaluator_aggregate_results:
            f.write(f"{i['file']}\n")
            f.write(f"{i}\n")
else:
    with open("evaluation_outputs/captured_relations_results/results.txt", "w") as f:
        f.write(f"New single file evaluation")
    INPUT_FILE_PATH = pathlib.Path("evaluation_datasets/multi_relation_dataset/test_paper_2.json")
    result = evaluate_one_paper(INPUT_FILE_PATH, settings_path=SETTINGS_PATH, verbose=True, debug_path=DEBUG_PATH)

========== END OF evaluations/captured_relations_evaluator.py ==========

(Directory) evaluations/test_inputs
(Code File) evaluations/multi_relation_evaluator.py


========== START OF evaluations/multi_relation_evaluator.py ==========

"""
General purpose evaluator should take in a YAML file which specifies the parameters: type of model, evaluation dataset, type of scoring function, and supporting tools combination
Run the appropriate pipeline with these parameters.
Calculate the score.
Save results, score, predictions(inputs and outputs), other relevant information, in evaluation outputs.
"""
import pathlib
from pipelines.multi_paper_pipeline import extract_relationships
import json
import os
from copy import deepcopy

mypath = os.path.abspath("")
print("___\n\n\n", mypath)

def compare(prediction, ground_truth):
    score_dictionary = dict()
    if prediction["RelationshipClassification"].lower() == ground_truth["RelationshipClassification"].lower():
        score_dictionary["RelationshipClassificationScore"] = 1
        print("RelationshipClassification Score ==> success;", ground_truth["RelationshipClassification"])
    else:
        score_dictionary["RelationshipClassificationScore"] = 0
        print("RelationshipClassification Score ==> actual:", ground_truth["RelationshipClassification"], "; predicted:", prediction["RelationshipClassification"])
    
    if prediction["isCausal"].lower() == ground_truth["isCausal"].lower():
        score_dictionary["isCausalScore"] = 1
        print("Causal Score ==> success; ", ground_truth["isCausal"])
    else:
        score_dictionary["isCausalScore"] = 0
        print("Causal Score ==> actual:", ground_truth["isCausal"], "; predicted:", prediction["isCausal"])
    return score_dictionary

# Read a YAML file
dataset_path = pathlib.Path("evaluation_datasets/multi_relation_dataset")

# Read evaluation dataset
with open(dataset_path / "test_paper.json") as f:
    ground_truth = json.load(f)
    print("datatype of ground truth", type(ground_truth))    

# Determine Settings
text = ground_truth["text"]
# extract_relationships based on settings (which is text and nothing else)
if True:
    predictions = extract_relationships(text).dict()
else:
    predictions = deepcopy(ground_truth)
predictions["Relations"][0]["RelationshipClassification"] = "inverse"
print("datatype of prediction", type(predictions))
# compare to obtain score

if len(predictions["Relations"]) > len(ground_truth["Relations"]):
    index_max = len(ground_truth["Relations"])
else: index_max = len(predictions["Relations"])

results: list(dict()) = list()
for relation_index in range(index_max):
    relation = ground_truth["Relations"][relation_index]
    prediction = predictions["Relations"][relation_index]
    results.append(compare(prediction, relation))

aggregate_results = dict()
aggregate_results["RelationshipClassificationScore"] = 0
aggregate_results["CausalIdentificationScore"] = 0
for x, result in enumerate(results):
    aggregate_results["RelationshipClassificationScore"] += result["RelationshipClassificationScore"]
    aggregate_results["CausalIdentificationScore"] += result["isCausalScore"]
aggregate_results["RelationshipClassificationScore"] /= (x + 1)
aggregate_results["CausalIdentificationScore"] /= (x + 1)
print(aggregate_results)

with open("evaluation_outputs/multi_relation_results/results.txt", "w") as f:
    f.write("Predictions\n")
    f.write(json.dumps(predictions, indent=2))
    f.write("Ground Truth\n")
    f.write(json.dumps(ground_truth, indent=2))
    f.write("Results\n")
    f.write(json.dumps(aggregate_results, indent=2))

========== END OF evaluations/multi_relation_evaluator.py ==========

(Directory) evaluations/ontology_methods
(Directory) evaluations/pipeline_parser
(Code File) evaluations/pipeline_parser/parser_old.py


========== START OF evaluations/pipeline_parser/parser_old.py ==========

import json, sys

# # Get the path to the output directory
# outputPath = sys.path[0][:-15] + "outputs\\"

# # Read the contents of the MultiVariablePipelineOutput.txt file
# with open(path + "MultiVariablePipelineOutput.txt") as f:
#     data = f.read()

def pipeline_to_kumu(f, outputPath):
    data = f.read()
    jsons = []
    first = 0
    file = ""

    # Split the data by newline character and process each line
    for line in data.split("\n"):
        # Check if the line indicates the start of a new JSON object
        if line[:10] == "pre parse:" or line[:31] == "successful parse MULTIRELATION:":
            # If there are already JSON objects in the list or it's not the first object, append the previous object to the list
            if(len(jsons) != 0 or first == 1):
                jsons.append(file)
                file = "{"
            else:
                file += "{"
                first = 1
        else:
            file += line
    
    vars = []
    connections = []
    elementList = []
    connectionList = []

    # Process each JSON object in the list

    for jsn in jsons:
        try:
            temp = json.loads(jsn)
            # Extract the variables and connections from each relation in the JSON object
            for relation in temp['Relations']:
                variable_one = relation['VariableOneName']
                variable_two = relation['VariableTwoName']
                relationType = relation['RelationshipClassification']
                SupportingText = relation['SupportingText']
                if relation['isCausal'] == "True":
                    relationType = "causal"
                elif relation['isCausal'] == "False":
                    relationType = "non-causal"
                # Add the variables to the list if they are not already present
                if(variable_one not in vars):
                    vars.append(variable_one)
                if(variable_two not in vars):
                    vars.append(variable_two)
                
                connections.append([variable_one, variable_two, relationType, SupportingText, relationType])
        except:
            print(jsons.index(jsn))
            

    # Create the element list for the output JSON
    for var in vars:
        entry = {
            "label": var,
            "type": "variable",
        }
        elementList.append(entry)
        
    # Create the connection list for the output JSON

    for connection in connections:
        if connection[2] == "direct" or connection[2] == "Direct":
            connection[2] = "directed"
        else:
            connection[2] = "mutual"
        entry = {
            "from": connection[0],
            "to": connection[1],
            "direction": connection[2],
            "type": connection[4],
            "description": connection[3]
        }
        connectionList.append(entry)

    # Write the output JSON to ParsedMultiVariablePipelineOutput.json file
    with open(outputPath + "ParsedMultiVariablePipelineOutput.json", 'w') as g:
        output_dict = {"elements": elementList, "connections": connectionList}
        json_str = json.dumps(output_dict, indent=4)
        g.write(json_str)
    g.close()

if __name__ == "__main__":
    outputPath = sys.path[0][:-15] + "outputs\\"
    f = open(outputPath + "MultiVariablePipelineOutput.txt")
    pipeline_to_kumu(f, outputPath)
        

========== END OF evaluations/pipeline_parser/parser_old.py ==========

(Code File) evaluations/pipeline_parser/__init__.py


========== START OF evaluations/pipeline_parser/__init__.py ==========



========== END OF evaluations/pipeline_parser/__init__.py ==========

(Directory) evaluations/pipeline_parser/__pycache__
(Code File) evaluations/pipeline_parser/parser.py


========== START OF evaluations/pipeline_parser/parser.py ==========

import json, sys

def pipeline_to_kumu(dic, outputPath):


    vars = []
    connections = []
    elementList = []
    connectionList = []
    verdicts = {}
    # Process each JSON object in the list

    for paper in dic["Papers"]:
        title = paper['PaperTitle']
        doi = paper['PaperDOI']

        # Extract the variables and connections from each relation in the JSON object
        for relation in paper['Relations']:
            variable_one = relation['VariableOneName']
            variable_two = relation['VariableTwoName']
            relationType = relation['RelationshipClassification']
            SupportingText = relation['SupportingText']
            if "isCausal" in relation:
                if relation['isCausal'] == "True":
                    isCausal = "causal"
                elif relation['isCausal'] == "False":
                    isCausal = "non-causal"
            else:
                isCausal = ""
            # Add the variables to the list if they are not already present
            if(variable_one not in vars):
                vars.append(variable_one)
            if(variable_two not in vars):
                vars.append(variable_two)
            
            if (variable_one, variable_two) not in verdicts:
                verdicts[(variable_one,variable_two)] = []
                verdicts[(variable_one,variable_two)].append( {"title": title, "doi": doi, "relationType": relationType, "isCausal": isCausal, "SupportingText": SupportingText})
            else:
                verdicts[(variable_one,variable_two)].append({"title": title, "doi": doi, "relationType": relationType, "isCausal": isCausal, "SupportingText": SupportingText})

            connections.append([variable_one, variable_two])

            
    #print(verdicts)
    # Create the element list for the output JSON
    for var in vars:
        entry = {
            "label": var,
            "type": "variable",
        }
        elementList.append(entry)
        
    # Create the connection list for the output JSON

    for connection in connections:
        # if connection[2] == "direct" or connection[2] == "Direct":
        #     connection[2] = "directed"
        # else:
        #     connection[2] = "mutual"
    
        entry = {
            "from": connection[0],
            "to": connection[1],
            #"direction": connection[2],
            #"type": connection[4],
            "description": '\n=====\n'.join([str(i) for i in verdicts[(connection[0], connection[1])]])
        }
        connectionList.append(entry)

    # Write the output JSON to ParsedMultiVariablePipelineOutput.json file
    with open(outputPath + "ParsedMultiVariablePipelineOutput.json", 'w') as g:
        output_dict = {"elements": elementList, "connections": connectionList}
        json_str = json.dumps(output_dict, indent=4)
        g.write(json_str)
    g.close()

# if __name__ == "__main__":
#     outputPath = sys.path[0][:-15] + "outputs\\"
#     f = open(outputPath + "MultiVariablePipelineOutput.txt")
#     pipeline_to_kumu(f, outputPath)
        

========== END OF evaluations/pipeline_parser/parser.py ==========

(Directory) evaluations/evaluation_datasets
(Directory) evaluations/evaluation_datasets/single_relation_dataset
(Directory) evaluations/evaluation_datasets/multi_relation_dataset
