(Directory) evaluations/evaluation_outputs
(Directory) evaluations/evaluation_outputs/results_and_predictions
(Code File) evaluations/__init__.py


========== START OF evaluations/__init__.py ==========

import sys
import pathlib
# get working directory
sys.path.append(pathlib.Path("../pipelines/"))

========== END OF evaluations/__init__.py ==========

(Directory) evaluations/pipelines
(Code File) evaluations/pipelines/__init__.py


========== START OF evaluations/pipelines/__init__.py ==========



========== END OF evaluations/pipelines/__init__.py ==========

(Directory) evaluations/pipelines/__pycache__
(Code File) evaluations/pipelines/test.py


========== START OF evaluations/pipelines/test.py ==========

# Test if you can successfully create and use a class defined in a function to make a pydantic object which you then use outside of the funciton

========== END OF evaluations/pipelines/test.py ==========

(Code File) evaluations/pipelines/multi_paper_pipeline.py


========== START OF evaluations/pipelines/multi_paper_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

# Configure constants
PAPER_SOURCE = pathlib.Path("../../papers")
OUTPUTS_SOURCE = pathlib.Path("../../outputs")

# Configure output parser classes
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    RelationshipClassification: str
    isCausal: str
    SupportingText: str

    @validator("RelationshipClassification")
    def question_ends_with_question_mark(cls, field):
        if field.lower() in {"direct", "inverse", "inconclusive"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

class ListOfRelations(BaseModel):
    Relations: list[SingleRelation]

def extract_relationships(text, verbose = False):
    # Add map reduce or some other type of summarization function here.
    processed_text = text

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=ListOfRelations) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    formatting_text = """
    The output should be formatted as a JSON instance that conforms to the JSON schema below.

    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

    Here is the output schema:
    ```
    {"Relationships":[{"properties": {"VariableOneName": {"title": "Variableonename", "type": "string"}, "VariableTwoName": {"title": "Variabletwoname", "type": "string"}, "RelationshipClassification": {"title": "Relationshipclassification", "type": "string"}, "isCausal": {"title": "Iscausal", "type": "string"}, "SupportingText": {"title": "Supportingtext", "type": "string"}}, "required": ["VariableOneName", "VariableTwoName", "RelationshipClassification", "isCausal", "SupportingText"]}]}
    ```"""
    prompt = PromptTemplate(
        template="""
        {text}

        Given the text identify a series of relationships between variables that have a well defined positive direction and a clear textual backing.
        For example, if a text says "We find a strong correlation p < .0001 between a country's per capita income and it's literacy rate" then "Country's per capita income" and "Country's literacy rate" would be good variables. Variables that are implied but not directly stated by the text such as "education infilstructure" should not be included. 
        The RelationshipClassification field can only be 'direct', 'inverse', 
        or 'inconclusive'. The isCausal field can only be either 'True' or 'False', and can only be true if the text directly states that the relationship is a causal relationship.
        The SupportText field of your output should include a section of verbatim from the text in addition to any comments you want to make about your output.
        Use exactly wording of outputs choices and input variable names, including capitalization choices.

        {format_instructions}
        """,
        input_variables=["text"],
        partial_variables={"format_instructions":parser.get_format_instructions}
    )
    input_text = prompt.format_prompt(text=processed_text).to_string()
    if verbose:
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineInput.txt", "a") as f:
            f.write("Input begins:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    if verbose:
        print("what is human_message_prompt:", type(human_message_prompt))
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    if verbose:
        print("what is chat_prompt:", type(chat_prompt))
    completion_prompt = chat_prompt.format_prompt(text=processed_text).to_messages()
    if verbose:
        print("What is a completion_prompt:", type(completion_prompt))

    # Create LLM
    model = ChatOpenAI(temperature=.3, openai_api_key=key, model_name="gpt-3.5-turbo-16k")

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        print("what is a output:", type(output), output.content)
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
            f.write("pre parse: ")
            f.write(str(output.content))
            f.write("\n")
    output = parser.parse(output.content)
    return output

if __name__ == "__main__":
    # Prepare inputs:
    text = str()
    with open(PAPER_SOURCE / "testpaper.txt") as f:
        text = f.read()
    variable_one = "AI/AN status"
    variable_two = "Substance use"

    # Process and save outputs:
    output = extract_relationships(text, verbose=True)
    with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
        f.write("successful parse MULTIRELATION: ")
        f.write(output.json())
        f.write("\n")

========== END OF evaluations/pipelines/multi_paper_pipeline.py ==========

(Code File) evaluations/pipelines/single_variable_pipeline.py


========== START OF evaluations/pipelines/single_variable_pipeline.py ==========

import langchain as lc
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import PydanticOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import openai
import pathlib
import os

# Configure OpenAI API key
from dotenv import load_dotenv
load_dotenv()
key = os.getenv("OPENAI_API_KEY")
openai.api_key = key

# Configure constants
PAPER_SOURCE = pathlib.Path("../../papers")
OUTPUTS_SOURCE = pathlib.Path("../../outputs")

# Configure output parser classes
class SingleRelation(BaseModel):
    VariableOneName: str
    VariableTwoName: str
    RelationshipClassification: str
    isCausal: str
    SupportingText: str

    @validator("RelationshipClassification")
    def question_ends_with_question_mark(cls, field):
        if field.lower() in {"direct", "inverse", "inconclusive"}:
            return field
        else:
            raise ValueError(f"Invalid Relationship Type {{{field}}}")

def extract_relationships(text, variable_one, variable_two, verbose = False):
    # Add map reduce or some other type of summarization function here.
    processed_text = text

    # Create Parser
    parser = PydanticOutputParser(pydantic_object=SingleRelation) #Refers to a class called SingleRelation

    # Create the plain text prompt. Used some of langchain's functions to automatically create formated prompts. 
    prompt = PromptTemplate(
        template = """
        {text}
        
        Given the text, identify the relationship between {variable_one} 
        and {variable_two}.
        
        {format_instructions}

        The RelationshipClassification field can only be 'direct', 'inverse', 
        or 'inconclusive'. The isCausal field can only be either 'True' or 'False', and can only be 
        true if the text directly states that the relationship is a causal relationship.
        The SupportText field of your output should include a section 
        of verbatim from the text in addition to any comments you want to make about your output.
        Use exactly wording of outputs choices and input variable names, including capitalization.""",
        input_variables=["text", "variable_one", "variable_two"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    input_text = prompt.format_prompt(text=processed_text, variable_one=variable_one, variable_two=variable_two).to_string()
    if verbose:
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineInput.txt", "a") as f:
            f.write("Input begins:\n")
            f.write(input_text)
            f.write("\n\n\n")
    human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)
    if verbose:
        print("what is human_message_prompt:", type(human_message_prompt))
    chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])
    if verbose:
        print("what is chat_prompt:", type(chat_prompt))
    completion_prompt = chat_prompt.format_prompt(text=processed_text, variable_one=variable_one, variable_two=variable_two).to_messages()
    if verbose:
        print("What is a completion_prompt:", type(completion_prompt))

    # Create LLM
    model = ChatOpenAI(temperature=.3, openai_api_key=key, model_name="gpt-3.5-turbo-16k")

    # Obtain completion from LLM
    output = model(completion_prompt)
    if verbose:
        print("what is a output:", type(output), output.content)
        with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
            f.write("pre parse: ")
            f.write(str(output.content))
            f.write("\n")
    output = parser.parse(output.content)
    return output

if __name__ == "__main__":
    # Prepare inputs:
    text = str()
    with open(PAPER_SOURCE / "testpaper.txt") as f:
        text = f.read()
    variable_one = "AI/AN status"
    variable_two = "Substance use"

    # Process and save outputs:
    output = extract_relationships(text, variable_one, variable_two, verbose=True)
    with open(OUTPUTS_SOURCE / "SingleVariablePipelineOutput.txt", "a") as f:
        f.write("successful parse: ")
        f.write(str(type(output.json()))+output.json())
        f.write("\n")

========== END OF evaluations/pipelines/single_variable_pipeline.py ==========

(Code File) evaluations/single_relation_evaluator.py


========== START OF evaluations/single_relation_evaluator.py ==========

"""
General purpose evaluator should take in a YAML file which specifies the parameters: type of model, evaluation dataset, type of scoring function, and supporting tools combination
Run the appropriate pipeline with these parameters.
Calculate the score.
Save results, score, predictions(inputs and outputs), other relevant information, in evaluation outputs.
"""
import pathlib
from pipelines.single_variable_pipeline import extract_relationships
import json
import os

mypath = os.path.abspath("")
print("___\n\n\n", mypath)

# Read a YAML file
dataset_path = pathlib.Path("evaluation_datasets/single_relation_dataset")

# Read evaluation dataset
with open(dataset_path / "1.json") as f:
    ground_truth = json.load(f)
    print("datatype of ground truth", type(ground_truth))

# Determine Settings
text = ground_truth["text"]
VariableOneName = ground_truth["VariableOneName"]
VariableTwoName = ground_truth["VariableTwoName"]
RelationshipClassification = ground_truth["RelationshipClassification"]
isCausal = ground_truth["isCausal"]

# extract_relationships()
# prediction = extract_relationships(text, VariableOneName, VariableTwoName).dict()
prediction = ground_truth.copy()
prediction["RelationshipClassification"] = "inverse"
print("datatype of prediction", type(prediction))
# compare to obtain score
def compare(prediction, ground_truth):
    score_dictionary = dict()
    if prediction["RelationshipClassification"] == ground_truth["RelationshipClassification"]:
        score_dictionary["RelationshipClassificationScore"] = 1
        print("RelationshipClassification Score ==> success")
    else:
        score_dictionary["RelationshipClassificationScore"] = 0
        print("RelationshipClassification Score ==> actual:", ground_truth["RelationshipClassification"], "; predicted:", prediction["RelationshipClassification"])
    
    if prediction["isCausal"] == ground_truth["isCausal"]:
        score_dictionary["isCausalScore"] = 1
        print("Causal Score ==> success")
    else:
        score_dictionary["isCausalScore"] = 0
        print("Causal Score ==> actual:", ground_truth["isCausal"], "; predicted:", prediction["isCausal"])

compare(prediction, ground_truth)

========== END OF evaluations/single_relation_evaluator.py ==========

(Directory) evaluations/__pycache__
(Code File) evaluations/multi_relation_evaluator.py


========== START OF evaluations/multi_relation_evaluator.py ==========

"""
General purpose evaluator should take in a YAML file which specifies the parameters: type of model, evaluation dataset, type of scoring function, and supporting tools combination
Run the appropriate pipeline with these parameters.
Calculate the score.
Save results, score, predictions(inputs and outputs), other relevant information, in evaluation outputs.
"""
import pathlib
from pipelines.multi_paper_pipeline import extract_relationships
import json
import os
from copy import deepcopy

mypath = os.path.abspath("")
print("___\n\n\n", mypath)

def compare(prediction, ground_truth):
    score_dictionary = dict()
    if prediction["RelationshipClassification"].lower() == ground_truth["RelationshipClassification"].lower():
        score_dictionary["RelationshipClassificationScore"] = 1
        print("RelationshipClassification Score ==> success;", ground_truth["RelationshipClassification"])
    else:
        score_dictionary["RelationshipClassificationScore"] = 0
        print("RelationshipClassification Score ==> actual:", ground_truth["RelationshipClassification"], "; predicted:", prediction["RelationshipClassification"])
    
    if prediction["isCausal"].lower() == ground_truth["isCausal"].lower():
        score_dictionary["isCausalScore"] = 1
        print("Causal Score ==> success; ", ground_truth["isCausal"])
    else:
        score_dictionary["isCausalScore"] = 0
        print("Causal Score ==> actual:", ground_truth["isCausal"], "; predicted:", prediction["isCausal"])
    return score_dictionary

# Read a YAML file
dataset_path = pathlib.Path("evaluation_datasets/multi_relation_dataset")

# Read evaluation dataset
with open(dataset_path / "test_paper.json") as f:
    ground_truth = json.load(f)
    print("datatype of ground truth", type(ground_truth))    

# Determine Settings
text = ground_truth["text"]
# extract_relationships based on settings (which is text and nothing else)
if True:
    predictions = extract_relationships(text).dict()
else:
    predictions = deepcopy(ground_truth)
predictions["Relations"][0]["RelationshipClassification"] = "inverse"
print("datatype of prediction", type(predictions))
# compare to obtain score

if len(predictions["Relations"]) > len(ground_truth["Relations"]):
    index_max = len(ground_truth["Relations"])
else: index_max = len(predictions["Relations"])

results: list(dict()) = list()
for relation_index in range(index_max):
    relation = ground_truth["Relations"][relation_index]
    prediction = predictions["Relations"][relation_index]
    results.append(compare(prediction, relation))

aggregate_results = dict()
aggregate_results["RelationshipClassificationScore"] = 0
aggregate_results["CausalIdentificationScore"] = 0
for x, result in enumerate(results):
    aggregate_results["RelationshipClassificationScore"] += result["RelationshipClassificationScore"]
    aggregate_results["CausalIdentificationScore"] += result["isCausalScore"]
aggregate_results["RelationshipClassificationScore"] /= (x + 1)
aggregate_results["CausalIdentificationScore"] /= (x + 1)
print(aggregate_results)

with open("evaluation_outputs/results_and_predictions/results.txt", "w") as f:
    f.write("Predictions\n")
    f.write(json.dumps(predictions, indent=2))
    f.write("Ground Truth\n")
    f.write(json.dumps(ground_truth, indent=2))
    f.write("Results\n")
    f.write(json.dumps(aggregate_results, indent=2))

========== END OF evaluations/multi_relation_evaluator.py ==========

(Directory) evaluations/evaluation_datasets
(Directory) evaluations/evaluation_datasets/single_relation_dataset
(Directory) evaluations/evaluation_datasets/multi_relation_dataset
